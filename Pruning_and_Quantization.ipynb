{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yFf6DN2UVUg"
      },
      "source": [
        "# *IT00CH92 Embedded AI - Spring 2024*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Pruning and Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEAZYXvZU_XG"
      },
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "yJwIonXEVJo6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "import tensorflow_datasets as tfds\n",
        "import matplotlib.pyplot as plt\n",
        "tf.random.set_seed(\n",
        "    seed=81\n",
        ")\n",
        "\n",
        "import tempfile\n",
        "from tensorflow_model_optimization.python.core.keras.compat import keras\n",
        "import tensorflow_model_optimization as tfmot\n",
        "# from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psViY5PRDurp"
      },
      "source": [
        "## Loading Tensorflow Datasets API to load MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wmSQZ-tC-Df"
      },
      "source": [
        "TFDS datasets often come with data already split into different sets. For MNIST, it has splits for train and test. We use the [Slicing API](https://www.tensorflow.org/datasets/splits#slicing_api) for TFDS to create a validation split.\n",
        "\n",
        "Next, we aim to understand how the dataset is formatted and utilize visualizations. Finally, the dataset is preprocessed before being passed to the model. For preprocessing, we simply normalize the image values as float32 within the range [0, 1] for all three splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset, info = tfds.load('mnist', with_info=True, as_supervised=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "a7217ee4cab547f5bd35e08dc5f0bb5c",
            "abbbabf618b340ca98c4b623f5582792",
            "036928fe21ff4d7381aa8d7c02f39089",
            "e36546f6b8ea4d69a8b70a51153447c7",
            "8645cf9cdddd4d72bebbbb0da0005f3d",
            "94ce1a37e8a6457788337f1f48c7e2fb",
            "5844b2244607427d92350e7c0327ccba",
            "66ef62feaa4f4738b011dc3973f08e25",
            "1e60b011eeae4df8801b8303d7f79cc6",
            "d460425ff77b42c781d2fd475574541b",
            "278df0c98469440fa669af97e1065f90"
          ]
        },
        "id": "D2qa8aaB3y5i",
        "outputId": "2dc84f2e-39fa-478d-e984-a2197ce2b851"
      },
      "outputs": [],
      "source": [
        "(ds_train, ds_val, ds_test), ds_info = tfds.load(\n",
        "    'mnist',\n",
        "    split=['train[:90%]', 'train[90%:]', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMPLTPUMHVv2"
      },
      "source": [
        "### Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "sbrdXWdb44HR"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "n_epochs = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb81aTYGHgOH"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "hsE6J4wn4ZfK"
      },
      "outputs": [],
      "source": [
        "def normalize_img(image:tf.uint8, label:tf.int64):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  return tf.cast(image, tf.float32) / 255., label\n",
        "\n",
        "def normalize_splits(ds, split_name: str, batch_size: int):\n",
        "  \"\"\"Applies preprocessing to train, val and test sets\"\"\"\n",
        "  ds = ds.map(\n",
        "    normalize_img, num_parallel_calls=tf.data.AUTOTUNE\n",
        "  )\n",
        "  ds = ds.cache() # Caching makes it faster for consecutive runs\n",
        "  if split_name != 'test':\n",
        "    # Shuffling is not done for the test set\n",
        "    ds = ds.shuffle(ds_info.splits[split_name].num_examples)\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "UhW9GjoGHmWu"
      },
      "outputs": [],
      "source": [
        "ds_train = normalize_splits(ds_train, split_name='train[:90%]', batch_size=batch_size)\n",
        "ds_val = normalize_splits(ds_val, split_name='train[90%:]', batch_size=batch_size)\n",
        "ds_test = normalize_splits(ds_test, split_name='test', batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading presaved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keras_file = 'saved_model/Full_Precision_MNIST_TF.h5'\n",
        "model = keras.models.load_model(keras_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Saving full model as TensorFlow Lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the converted model to a file\n",
        "tflite_model_file = 'saved_model/Full_Precision_MNIST_TF.tflite'\n",
        "with open(tflite_model_file, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(f\"TensorFlow Lite model saved as {tflite_model_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pruning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-tune pre-trained model with pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " prune_low_magnitude_conv2d  (None, 28, 28, 6)         308       \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            " prune_low_magnitude_max_po  (None, 14, 14, 6)         1         \n",
            " oling2d (PruneLowMagnitude                                      \n",
            " )                                                               \n",
            "                                                                 \n",
            " prune_low_magnitude_conv2d  (None, 14, 14, 16)        4818      \n",
            " _1 (PruneLowMagnitude)                                          \n",
            "                                                                 \n",
            " prune_low_magnitude_max_po  (None, 7, 7, 16)          1         \n",
            " oling2d_1 (PruneLowMagnitu                                      \n",
            " de)                                                             \n",
            "                                                                 \n",
            " prune_low_magnitude_flatte  (None, 784)               1         \n",
            " n (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_dense   (None, 120)               188282    \n",
            " (PruneLowMagnitude)                                             \n",
            "                                                                 \n",
            " prune_low_magnitude_dense_  (None, 84)                20246     \n",
            " 1 (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            " prune_low_magnitude_dense_  (None, 10)                1692      \n",
            " 2 (PruneLowMagnitude)                                           \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 215349 (841.24 KB)\n",
            "Trainable params: 107786 (421.04 KB)\n",
            "Non-trainable params: 107563 (420.20 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
        "\n",
        "num_images = len(ds_train)\n",
        "end_step = np.ceil(num_images / batch_size).astype(np.int32) * epochs\n",
        "\n",
        "# Define model for pruning.\n",
        "pruning_params = {\n",
        "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
        "                                                               final_sparsity=0.80,\n",
        "                                                               begin_step=0,\n",
        "                                                               end_step=end_step)\n",
        "}\n",
        "\n",
        "\n",
        "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
        "\n",
        "# `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_for_pruning.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train and evaluate the model against baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mariamaoliveira/anaconda3/envs/embedded_ai/lib/python3.10/site-packages/tf_keras/src/backend.py:5729: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  6/422 [..............................] - ETA: 4s - loss: 0.0294 - accuracy: 0.9935   WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0098s vs `on_train_batch_end` time: 0.0273s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0098s vs `on_train_batch_end` time: 0.0273s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "422/422 [==============================] - 7s 12ms/step - loss: 0.0398 - accuracy: 0.9876 - val_loss: 0.0505 - val_accuracy: 0.9835\n",
            "Epoch 2/2\n",
            "422/422 [==============================] - 5s 11ms/step - loss: 0.0318 - accuracy: 0.9901 - val_loss: 0.0479 - val_accuracy: 0.9862\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf_keras.src.callbacks.History at 0x301bfc220>"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "]\n",
        "\n",
        "model_for_pruning.fit(ds_train,\n",
        "                  batch_size=batch_size, epochs=epochs, validation_data=ds_val,\n",
        "                  callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline test accuracy: 0.9829999804496765\n",
            "Pruned test accuracy: 0.9887999892234802\n"
          ]
        }
      ],
      "source": [
        "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
        "   ds_test, verbose=0)\n",
        "\n",
        "print('Baseline test accuracy:', 0.9829999804496765) \n",
        "print('Pruned test accuracy:', model_for_pruning_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create 3x smaller models from pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/ipykernel_7470/2832786679.py:7: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved pruned Keras model to: saved_model/pruned_model_1.h5\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Do the pruning\n",
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
        "\n",
        "# _, pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "\n",
        "pruned_keras_file = \"saved_model/pruned_model_1.h5\"\n",
        "keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
        "print('Saved pruned Keras model to:', pruned_keras_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpmme71_4b/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpmme71_4b/assets\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved pruned TFLite model to: saved_model/pruned_model_tflite_1.tflite\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0000 00:00:1717498141.837855 13438389 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
            "W0000 00:00:1717498141.837864 13438389 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
            "2024-06-04 13:49:01.838002: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpmme71_4b\n",
            "2024-06-04 13:49:01.838622: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
            "2024-06-04 13:49:01.838628: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpmme71_4b\n",
            "2024-06-04 13:49:01.844105: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
            "2024-06-04 13:49:01.857470: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpmme71_4b\n",
            "2024-06-04 13:49:01.863397: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 25396 microseconds.\n"
          ]
        }
      ],
      "source": [
        "# Converting to TensorFlowLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "pruned_tflite_model = converter.convert()\n",
        "\n",
        "# _, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "pruned_tflite_file = \"saved_model/pruned_model_tflite_1.tflite\"\n",
        "\n",
        "with open(pruned_tflite_file, 'wb') as f:\n",
        "  f.write(pruned_tflite_model)\n",
        "\n",
        "print('Saved pruned TFLite model to:', pruned_tflite_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_gzipped_model_size(file):\n",
        "  # Returns size of gzipped model, in bytes.\n",
        "  import os\n",
        "  import zipfile\n",
        "\n",
        "  # _, zipped_file = tempfile.mkstemp('.zip')\n",
        "  zipped_file = \"saved_model/pruned_model_zipped_1.zip\"\n",
        "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
        "    f.write(file)\n",
        "\n",
        "  return os.path.getsize(zipped_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of gzipped baseline Keras model: 1200921.00 bytes\n",
            "Size of gzipped pruned Keras model: 402662.00 bytes\n",
            "Size of gzipped pruned TFlite model: 400753.00 bytes\n"
          ]
        }
      ],
      "source": [
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
        "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a 10x smaller model from combining pruning and quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Full integer quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pruned_model = keras.models.load_model(\"saved_model/pruned_model_1.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def representative_data_gen():\n",
        "  for image, label  in ds_train.take(100) :\n",
        "    yield [image]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpabbyaf4m/assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpabbyaf4m/assets\n",
            "/Users/mariamaoliveira/anaconda3/envs/embedded_ai/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:964: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\n",
            "W0000 00:00:1717498142.996554 13438389 tf_tfl_flatbuffer_helpers.cc:390] Ignored output_format.\n",
            "W0000 00:00:1717498142.996563 13438389 tf_tfl_flatbuffer_helpers.cc:393] Ignored drop_control_dependency.\n",
            "2024-06-04 13:49:02.996687: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpabbyaf4m\n",
            "2024-06-04 13:49:02.997481: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
            "2024-06-04 13:49:02.997487: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpabbyaf4m\n",
            "2024-06-04 13:49:03.002596: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.\n",
            "2024-06-04 13:49:03.016193: I tensorflow/cc/saved_model/loader.cc:218] Running initialization op on SavedModel bundle at path: /var/folders/s7/3gb_v_vn6ns01664c_jtg6hh0000gn/T/tmpabbyaf4m\n",
            "2024-06-04 13:49:03.021294: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 24608 microseconds.\n",
            "2024-06-04 13:49:05.857064: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
            "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# Set the input and output tensors to uint8 \n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "tflite_model_quant = converter.convert()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved quantized and pruned TFLite model to: saved_model/pruned_model_tflite_quantized_1.tflite\n",
            "Size of gzipped baseline Keras model: 1200921.00 bytes\n",
            "Size of gzipped pruned and quantized TFlite model: 97102.00 bytes\n"
          ]
        }
      ],
      "source": [
        "#Saving model\n",
        "_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "quantized_and_pruned_tflite_file = \"saved_model/pruned_model_tflite_quantized_1.tflite\"\n",
        "\n",
        "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
        "  f.write(tflite_model_quant)\n",
        "\n",
        "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
        "\n",
        "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
        "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check persistence of accuracy from TF to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fuction th\n",
        "def evaluate_model(interpreter):\n",
        "    input_index = interpreter.get_input_details()[0][\"index\"]\n",
        "    output_index = interpreter.get_output_details()[0][\"index\"]\n",
        " \n",
        "    prediction_digits = []\n",
        "    test_labels = []\n",
        "    \n",
        "\n",
        "    # Iterate over each batch in the test dataset\n",
        "    for k, (batch, labels) in enumerate(ds_test):\n",
        "\n",
        "        print('Evaluated on {n} results so far.'.format(n=k))\n",
        "        # Scale and converts to integer \n",
        "        batch = tf.cast(batch * 255.0, tf.uint8)\n",
        "\n",
        "        # Go through each batch\n",
        "        for i in range(batch.shape[0]):\n",
        "            test_image = batch[i:i+1]  \n",
        "            labels_set = labels[i:i+1]\n",
        "            test_labels.append(labels_set.numpy()[0]) \n",
        "    \n",
        "            # Ensure the input is in the correct format\n",
        "            interpreter.set_tensor(input_index, test_image.numpy())\n",
        "            # Run inference\n",
        "            interpreter.invoke()\n",
        "\n",
        "            # Get the output and find the predicted digit\n",
        "            output = interpreter.get_tensor(output_index)\n",
        "\n",
        "            digit = np.argmax(output, axis=1)\n",
        "            \n",
        "            prediction_digits.append(digit[0])\n",
        "    \n",
        "    # Calculate the accuracy by comparing predicted and true labels\n",
        "    prediction_digits = np.array(prediction_digits)\n",
        "    test_labels = np.array(test_labels)\n",
        "    accuracy = (prediction_digits == test_labels).mean()\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluated on 0 results so far.\n",
            "Evaluated on 1 results so far.\n",
            "Evaluated on 2 results so far.\n",
            "Evaluated on 3 results so far.\n",
            "Evaluated on 4 results so far.\n",
            "Evaluated on 5 results so far.\n",
            "Evaluated on 6 results so far.\n",
            "Evaluated on 7 results so far.\n",
            "Evaluated on 8 results so far.\n",
            "Evaluated on 9 results so far.\n",
            "Evaluated on 10 results so far.\n",
            "Evaluated on 11 results so far.\n",
            "Evaluated on 12 results so far.\n",
            "Evaluated on 13 results so far.\n",
            "Evaluated on 14 results so far.\n",
            "Evaluated on 15 results so far.\n",
            "Evaluated on 16 results so far.\n",
            "Evaluated on 17 results so far.\n",
            "Evaluated on 18 results so far.\n",
            "Evaluated on 19 results so far.\n",
            "Evaluated on 20 results so far.\n",
            "Evaluated on 21 results so far.\n",
            "Evaluated on 22 results so far.\n",
            "Evaluated on 23 results so far.\n",
            "Evaluated on 24 results so far.\n",
            "Evaluated on 25 results so far.\n",
            "Evaluated on 26 results so far.\n",
            "Evaluated on 27 results so far.\n",
            "Evaluated on 28 results so far.\n",
            "Evaluated on 29 results so far.\n",
            "Evaluated on 30 results so far.\n",
            "Evaluated on 31 results so far.\n",
            "Evaluated on 32 results so far.\n",
            "Evaluated on 33 results so far.\n",
            "Evaluated on 34 results so far.\n",
            "Evaluated on 35 results so far.\n",
            "Evaluated on 36 results so far.\n",
            "Evaluated on 37 results so far.\n",
            "Evaluated on 38 results so far.\n",
            "Evaluated on 39 results so far.\n",
            "Evaluated on 40 results so far.\n",
            "Evaluated on 41 results so far.\n",
            "Evaluated on 42 results so far.\n",
            "Evaluated on 43 results so far.\n",
            "Evaluated on 44 results so far.\n",
            "Evaluated on 45 results so far.\n",
            "Evaluated on 46 results so far.\n",
            "Evaluated on 47 results so far.\n",
            "Evaluated on 48 results so far.\n",
            "Evaluated on 49 results so far.\n",
            "Evaluated on 50 results so far.\n",
            "Evaluated on 51 results so far.\n",
            "Evaluated on 52 results so far.\n",
            "Evaluated on 53 results so far.\n",
            "Evaluated on 54 results so far.\n",
            "Evaluated on 55 results so far.\n",
            "Evaluated on 56 results so far.\n",
            "Evaluated on 57 results so far.\n",
            "Evaluated on 58 results so far.\n",
            "Evaluated on 59 results so far.\n",
            "Evaluated on 60 results so far.\n",
            "Evaluated on 61 results so far.\n",
            "Evaluated on 62 results so far.\n",
            "Evaluated on 63 results so far.\n",
            "Evaluated on 64 results so far.\n",
            "Evaluated on 65 results so far.\n",
            "Evaluated on 66 results so far.\n",
            "Evaluated on 67 results so far.\n",
            "Evaluated on 68 results so far.\n",
            "Evaluated on 69 results so far.\n",
            "Evaluated on 70 results so far.\n",
            "Evaluated on 71 results so far.\n",
            "Evaluated on 72 results so far.\n",
            "Evaluated on 73 results so far.\n",
            "Evaluated on 74 results so far.\n",
            "Evaluated on 75 results so far.\n",
            "Evaluated on 76 results so far.\n",
            "Evaluated on 77 results so far.\n",
            "Evaluated on 78 results so far.\n",
            "Pruned and quantized TFLite test_accuracy: 0.9889\n",
            "Pruned TF test accuracy: 0.9887999892234802\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-06-04 13:49:55.231208: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
          ]
        }
      ],
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "test_accuracy = evaluate_model(interpreter)\n",
        "\n",
        "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)\n",
        "print('Pruned TF test accuracy:', model_for_pruning_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convert the model to .h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Header file, model.h, is 698,052 bytes.\n",
            "\n",
            "Open the side panel (refresh if needed). Double click model.h to download the file.\n"
          ]
        }
      ],
      "source": [
        "!echo \"const unsigned char model[] = {\" > classifying_imu/content/model.h\n",
        "!cat saved_model/pruned_model_tflite_quantized_1.tflite | xxd -i      >> classifying_imu/content/model.h\n",
        "!echo \"};\"                              >> classifying_imu/content/model.h\n",
        "\n",
        "import os\n",
        "model_h_size = os.path.getsize(\"classifying_imu/content/model.h\")\n",
        "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "036928fe21ff4d7381aa8d7c02f39089": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66ef62feaa4f4738b011dc3973f08e25",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e60b011eeae4df8801b8303d7f79cc6",
            "value": 5
          }
        },
        "1e60b011eeae4df8801b8303d7f79cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "278df0c98469440fa669af97e1065f90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5844b2244607427d92350e7c0327ccba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66ef62feaa4f4738b011dc3973f08e25": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8645cf9cdddd4d72bebbbb0da0005f3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94ce1a37e8a6457788337f1f48c7e2fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7217ee4cab547f5bd35e08dc5f0bb5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_abbbabf618b340ca98c4b623f5582792",
              "IPY_MODEL_036928fe21ff4d7381aa8d7c02f39089",
              "IPY_MODEL_e36546f6b8ea4d69a8b70a51153447c7"
            ],
            "layout": "IPY_MODEL_8645cf9cdddd4d72bebbbb0da0005f3d"
          }
        },
        "abbbabf618b340ca98c4b623f5582792": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94ce1a37e8a6457788337f1f48c7e2fb",
            "placeholder": "​",
            "style": "IPY_MODEL_5844b2244607427d92350e7c0327ccba",
            "value": "Dl Completed...: 100%"
          }
        },
        "d460425ff77b42c781d2fd475574541b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e36546f6b8ea4d69a8b70a51153447c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d460425ff77b42c781d2fd475574541b",
            "placeholder": "​",
            "style": "IPY_MODEL_278df0c98469440fa669af97e1065f90",
            "value": " 5/5 [00:00&lt;00:00, 13.41 file/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
